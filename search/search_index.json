{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to LightNet \u00b6 LightNet is a light-weight neural network inference optimizer for different software/hardware backends. Getting Started \u00b6 Installation Usage Documentation \u00b6 Overview Project Structure Build Options Data Structures Intermediate Representation Operator Description Optimizer Description Miscellaneous Hacking \u00b6 Commit Guidelines Code Conventions","title":"Home"},{"location":"index.html#welcome-to-lightnet","text":"LightNet is a light-weight neural network inference optimizer for different software/hardware backends.","title":"Welcome to LightNet"},{"location":"index.html#getting-started","text":"Installation Usage","title":"Getting Started"},{"location":"index.html#documentation","text":"Overview Project Structure Build Options Data Structures Intermediate Representation Operator Description Optimizer Description Miscellaneous","title":"Documentation"},{"location":"index.html#hacking","text":"Commit Guidelines Code Conventions","title":"Hacking"},{"location":"Getting-Started.html","text":"Getting Started \u00b6 Installation \u00b6 Requirements \u00b6 The following steps have been tested for Ubuntu 16.04 but should work with other distros as well. Most required packages can be installed using the following commands ( sudo permission may be required): apt-get install build-essential perl git pkg-config check cpan install JSON Sort::strverscmp This project also depends on TensorLight , a lightweight tensor operation library. Install it according to its repository before continuing to build LightNet. (Optional) Packages for building documents Use the following commands to install the packages for building documents: apt-get install python3-pip pip3 install mkdocs (Optional) Packages for building python packages Use the following commands to install the packages for building python packages (python3 for example): apt-get install python3-setuptools (Optional) CUDA dependency You also have to install CUDA 8.0 (or later) according to its website CUDA Toolkit if you want to build with CUDA support. Remember to put nvcc (usually in /usr/local/cuda/bin ) in environment variable PATH . (Optional) cuDNN dependency You also have to install CUDA 8.0 (or later) and cuDNN 7.0 (or later) libraries, according to their websites CUDA Toolkit and cuDNN if you want to build with cuDNN support. (Optional) TensorRT dependency You also have to install CUDA 8.0 (or later) and TensorRT 3.0 (or later) libraries, according to their websites CUDA Toolkit and TensorRT if you want to build with TensorRT support. Building and Installation \u00b6 Clone this repository to your local directory. cd &lt;my_working_directory&gt; git clone https://github.com/zhaozhixu/LightNet.git cd LightNet Configure and build First, configure your installation using: chmod +x configure ./configure There are options to custom your building and installation process. You can append them after ./configure . For example, use ./configure --install-dir=DIR to set the installation directory (default is /usr/local ). Use ./configure --with-cuda=yes if you want to build with CUDA support. Detailed ./configure options can be displayed using ./configure -h . After that, use make to compile the binaries, and run the test. Finally, use make install to install the build directory into the installation directory. Other make options Use make info to see other make options. Especially, you can use make clean to clean up the build directory and all object files, and make uninstall to remove installed files from the installation directory. Usage \u00b6 After compilation, the following components will be installed: lightnet: LightNet command tool liblightnet.so: LightNet runtime library ir2json.pl: LightNet intermediate language (IL) interpreter pylightnet (optional): LightNet python wrapper package LightNet provides 3 interfaces which can be used by developers and users: Command line C API Python API Command line \u00b6 From the command line, type lightnet -h can display the program usage, as follows. Usage: lightnet [OPTION...] SOURCE Apply compilation procedures to SOURCE according to the options. If SOURCE is -, read standard input. Options: -h, --help display this message -v, --version display version information -o, --outfile=FILE specify output file name; if FILE is -, print to standard output; if FILE is !, do not print; (default: out.json) -t, --target=TARGET specify target platform (default: cpu) -f, --datafile=FILE specify tensor data file -c, --compile compile only; do not run -r, --run run only; do not compile; SOURCE should have been memory-planned -Wwarn display warnings (default) -w, -Wno-warn do not display warnings -Winter display internal warnings (default) -Wno-inter do not display internal warnings -debug display debug messages (only works with LN_DEBUG defined when compiling) C API \u00b6 Python API \u00b6","title":"Getting Started"},{"location":"Getting-Started.html#getting-started","text":"","title":"Getting Started"},{"location":"Getting-Started.html#installation","text":"","title":"Installation"},{"location":"Getting-Started.html#requirements","text":"The following steps have been tested for Ubuntu 16.04 but should work with other distros as well. Most required packages can be installed using the following commands ( sudo permission may be required): apt-get install build-essential perl git pkg-config check cpan install JSON Sort::strverscmp This project also depends on TensorLight , a lightweight tensor operation library. Install it according to its repository before continuing to build LightNet. (Optional) Packages for building documents Use the following commands to install the packages for building documents: apt-get install python3-pip pip3 install mkdocs (Optional) Packages for building python packages Use the following commands to install the packages for building python packages (python3 for example): apt-get install python3-setuptools (Optional) CUDA dependency You also have to install CUDA 8.0 (or later) according to its website CUDA Toolkit if you want to build with CUDA support. Remember to put nvcc (usually in /usr/local/cuda/bin ) in environment variable PATH . (Optional) cuDNN dependency You also have to install CUDA 8.0 (or later) and cuDNN 7.0 (or later) libraries, according to their websites CUDA Toolkit and cuDNN if you want to build with cuDNN support. (Optional) TensorRT dependency You also have to install CUDA 8.0 (or later) and TensorRT 3.0 (or later) libraries, according to their websites CUDA Toolkit and TensorRT if you want to build with TensorRT support.","title":"Requirements"},{"location":"Getting-Started.html#building-and-installation","text":"Clone this repository to your local directory. cd &lt;my_working_directory&gt; git clone https://github.com/zhaozhixu/LightNet.git cd LightNet Configure and build First, configure your installation using: chmod +x configure ./configure There are options to custom your building and installation process. You can append them after ./configure . For example, use ./configure --install-dir=DIR to set the installation directory (default is /usr/local ). Use ./configure --with-cuda=yes if you want to build with CUDA support. Detailed ./configure options can be displayed using ./configure -h . After that, use make to compile the binaries, and run the test. Finally, use make install to install the build directory into the installation directory. Other make options Use make info to see other make options. Especially, you can use make clean to clean up the build directory and all object files, and make uninstall to remove installed files from the installation directory.","title":"Building and Installation"},{"location":"Getting-Started.html#usage","text":"After compilation, the following components will be installed: lightnet: LightNet command tool liblightnet.so: LightNet runtime library ir2json.pl: LightNet intermediate language (IL) interpreter pylightnet (optional): LightNet python wrapper package LightNet provides 3 interfaces which can be used by developers and users: Command line C API Python API","title":"Usage"},{"location":"Getting-Started.html#command-line","text":"From the command line, type lightnet -h can display the program usage, as follows. Usage: lightnet [OPTION...] SOURCE Apply compilation procedures to SOURCE according to the options. If SOURCE is -, read standard input. Options: -h, --help display this message -v, --version display version information -o, --outfile=FILE specify output file name; if FILE is -, print to standard output; if FILE is !, do not print; (default: out.json) -t, --target=TARGET specify target platform (default: cpu) -f, --datafile=FILE specify tensor data file -c, --compile compile only; do not run -r, --run run only; do not compile; SOURCE should have been memory-planned -Wwarn display warnings (default) -w, -Wno-warn do not display warnings -Winter display internal warnings (default) -Wno-inter do not display internal warnings -debug display debug messages (only works with LN_DEBUG defined when compiling)","title":"Command line"},{"location":"Getting-Started.html#c-api","text":"","title":"C API"},{"location":"Getting-Started.html#python-api","text":"","title":"Python API"},{"location":"Hacking.html","text":"Hacking \u00b6 Commit Guidelines \u00b6 Submit an issue if you found a bug or have a feature request. Fork and open a pull request when you prepared to contribute. Before that, it is encouraged to open an issue to discuss. When committing to the git repository make sure to include a meaningful commit message. Commit messages should have the following format: Short explanation of the commit Longer explanation explaining exactly what's changed, whether any external or private interfaces changed, what bugs were fixed (with bug tracker reference if applicable) and so on. Be concise but not too brief. Code Conventions \u00b6 Indent the C code with 4 spaces. Use /* */ comments in the C code. The limit on the length of lines is 80 columns and this is a strongly preferred limit, unless exceeding 80 columns significantly increases readability. However, never break user-visible strings such as printf messages. Placing Braces Put the opening brace last on the line except namely functions. Put the closing brace first except in the cases where it is followed by a continuation of the same statement, ie a while in a do-statement or an else in an if-statement. Do not unnecessarily use braces where a single statement will do except that only one branch of a conditional statement is a single statement. E.g.: int function ( int x ) { if ( condition ) { do a ; do b ; } if ( condition ) do e ; if ( condition ) { do f ; do g ; } else { do h ; } do { do c ; do d ; } while ( condition ); } Spaces Use a space after control-flow keywords: if, switch, case, for, do, while . Use a space after each comma. Use spaces around most binary and ternary operators but no space after unary operators. Avoid trailing space. Don't hide pointers with typedef . Bad example: typedef struct foo * foo; As a general rule of thumb, follow the same coding style as the surrounding code.","title":"Hacking"},{"location":"Hacking.html#hacking","text":"","title":"Hacking"},{"location":"Hacking.html#commit-guidelines","text":"Submit an issue if you found a bug or have a feature request. Fork and open a pull request when you prepared to contribute. Before that, it is encouraged to open an issue to discuss. When committing to the git repository make sure to include a meaningful commit message. Commit messages should have the following format: Short explanation of the commit Longer explanation explaining exactly what's changed, whether any external or private interfaces changed, what bugs were fixed (with bug tracker reference if applicable) and so on. Be concise but not too brief.","title":"Commit Guidelines"},{"location":"Hacking.html#code-conventions","text":"Indent the C code with 4 spaces. Use /* */ comments in the C code. The limit on the length of lines is 80 columns and this is a strongly preferred limit, unless exceeding 80 columns significantly increases readability. However, never break user-visible strings such as printf messages. Placing Braces Put the opening brace last on the line except namely functions. Put the closing brace first except in the cases where it is followed by a continuation of the same statement, ie a while in a do-statement or an else in an if-statement. Do not unnecessarily use braces where a single statement will do except that only one branch of a conditional statement is a single statement. E.g.: int function ( int x ) { if ( condition ) { do a ; do b ; } if ( condition ) do e ; if ( condition ) { do f ; do g ; } else { do h ; } do { do c ; do d ; } while ( condition ); } Spaces Use a space after control-flow keywords: if, switch, case, for, do, while . Use a space after each comma. Use spaces around most binary and ternary operators but no space after unary operators. Avoid trailing space. Don't hide pointers with typedef . Bad example: typedef struct foo * foo; As a general rule of thumb, follow the same coding style as the surrounding code.","title":"Code Conventions"},{"location":"Documentation/Build-Options.html","text":"Build Options \u00b6 Configuration \u00b6 The LightNet source code must be configured before being built. This process uses a configure script in the project root. You can configure LightNet with the following invocation: chmod +x configure ./configure [&lt;option&gt;[=&lt;value&gt;]...] Various configuration options can be passed to configure on the command line. It then checks dependencies and print makefile variables in config.mk , which will then be included by other makefiles. Here is a list of the configuration options, which can also be printed by ./configure -h . Default options are showed in brackets. -h, --help print help information --target=<name> target name [ lightnet ] --abbr=<abbr> abbreviation name [ ln ] --build-dir=<path> building directory [ build ] --install-dir=<path> installation directory [ /usr/local ] --pkgconfig-dir=<path> pkgconfig directory [ /usr/local/lib/pkgconfig ] --with-cuda=<bool> set to yes if build with CUDA [ no ] --with-cudnn=<bool> set to yes if build with cudnn library [ no ] --cuda-install-dir=<path> cuda installation directory [ /usr/local/cuda ] --with-tensorrt=<bool> set to yes if build with TensorRT [ no ] --tensorrt-install-dir=<path> tensorrt installation directory [ /usr ] --debug=<bool> set to yes when debugging [ no ] --doc=<bool> set to yes if build the documents too [ yes ] Make Options \u00b6 Once you have LightNet configured, you can build it by issuing the following command: make If you have multiple processors in your machine, you may wish to use some of the parallel build options provided by GNU Make. For example, you could use the command: make -j2 There are several targets which are useful when working with the LightNet source code: make make all Compile binaries, compile and run tests, and build the documentation if configured with --doc=yes . Binaries, header files and documents will be generated in a hierarchy under the configured build directory, specified with --build-dir , which defaults to build . make bin Compile executables. make test Compile and run tests. make doc Build the documentation if configured with --doc=yes . make install Install the build directory under the configured install directory, specified with --install-dir , which defaults to /usr/local . make clean Remove all files generated by the build. This includes object files, generated C/C++ files, libraries, executables and documentation. make uninstall Remove all the files installed by make install . make info Show a brief information of make targets.","title":"Build Options"},{"location":"Documentation/Build-Options.html#build-options","text":"","title":"Build Options"},{"location":"Documentation/Build-Options.html#configuration","text":"The LightNet source code must be configured before being built. This process uses a configure script in the project root. You can configure LightNet with the following invocation: chmod +x configure ./configure [&lt;option&gt;[=&lt;value&gt;]...] Various configuration options can be passed to configure on the command line. It then checks dependencies and print makefile variables in config.mk , which will then be included by other makefiles. Here is a list of the configuration options, which can also be printed by ./configure -h . Default options are showed in brackets. -h, --help print help information --target=<name> target name [ lightnet ] --abbr=<abbr> abbreviation name [ ln ] --build-dir=<path> building directory [ build ] --install-dir=<path> installation directory [ /usr/local ] --pkgconfig-dir=<path> pkgconfig directory [ /usr/local/lib/pkgconfig ] --with-cuda=<bool> set to yes if build with CUDA [ no ] --with-cudnn=<bool> set to yes if build with cudnn library [ no ] --cuda-install-dir=<path> cuda installation directory [ /usr/local/cuda ] --with-tensorrt=<bool> set to yes if build with TensorRT [ no ] --tensorrt-install-dir=<path> tensorrt installation directory [ /usr ] --debug=<bool> set to yes when debugging [ no ] --doc=<bool> set to yes if build the documents too [ yes ]","title":"Configuration"},{"location":"Documentation/Build-Options.html#make-options","text":"Once you have LightNet configured, you can build it by issuing the following command: make If you have multiple processors in your machine, you may wish to use some of the parallel build options provided by GNU Make. For example, you could use the command: make -j2 There are several targets which are useful when working with the LightNet source code: make make all Compile binaries, compile and run tests, and build the documentation if configured with --doc=yes . Binaries, header files and documents will be generated in a hierarchy under the configured build directory, specified with --build-dir , which defaults to build . make bin Compile executables. make test Compile and run tests. make doc Build the documentation if configured with --doc=yes . make install Install the build directory under the configured install directory, specified with --install-dir , which defaults to /usr/local . make clean Remove all files generated by the build. This includes object files, generated C/C++ files, libraries, executables and documentation. make uninstall Remove all the files installed by make install . make info Show a brief information of make targets.","title":"Make Options"},{"location":"Documentation/Data-Structures.html","text":"Data Structures \u00b6 Linked List \u00b6 LightNet uses linked lists to manage a sequence of data structures, represented by ln_list , which is a simple singlely linked list. struct ln_list { void * data ; struct ln_list * next ; }; typedef struct ln_list ln_list ; ln_list doesn't have an empty header node, every node keeps real data. It has no \"create\" functions. Append an element to a NULL list to create a new list. Remember to always use the returned pointer as the newest list pointer, because the first node may have changed during updating (append, prepend, remove, insert, etc.). ln_list * list = NULL ; list = ln_list_append ( list , first_data ); ln_list supports the following operations: ln_list *ln_list_prepend(ln_list *list, void *data) Prepend an element at the beginning of the list. ln_list *ln_list_append(ln_list *list, void *data) Append an element at the end of the list. void ln_list_free(ln_list *list) Free all nodes in the list. void ln_list_free_deep(ln_list *list, void (*free_func)(void *)) Free all nodes int the list as well as their elements, which are freed by free_func . ln_list *ln_list_nth(ln_list *list, int n) Return the nth node in a list. void *ln_list_nth_data(ln_list *list, int n) Return the nth element in a list. ln_list *ln_list_remove(ln_list *list, void *data) Remove the first node that has data as its element in a list. data is compared directly by the pointer value. ln_list *ln_list_remove_nth(ln_list *list, int n) Remove the nth node. ln_list *ln_list_remove_nth_deep(ln_list *list, int n, void (*free_func)(void *)) Remove the nth node as well as its element, which is freed by free_func . ln_list *ln_list_remove_custom(ln_list *list, void *data, ln_cmp_func cmp) Remove the first node whose element can be compared with data by cmp returning 0. ln_list *ln_list_remove_custom_deep(ln_list *list, void *data, ln_cmp_func cmp, void (*free_func)(void *)) Remove the first node as well as its element which can be compared with data by cmp returning 0. The element is freed by free_func . ln_list *ln_list_remove_all_custom_deep(ln_list *list, void *data, ln_cmp_func cmp, void (*free_func)(void *)) Remove all the nodes as well as their elements which can be compared with data by cmp returning 0. The elements are freed by free_func . ln_list *ln_list_insert_before(ln_list *list, void *data, ln_list *node) Insert an element data before node in a list. ln_list *ln_list_insert_nth(ln_list *list, void *data, int n) Insert an element data at the nth position in a list. void *ln_list_find(ln_list *list, void *data) Find an element data in a list. data is compared directly by its pointer value. Return the element if it's found, else return NULL . void *ln_list_find_custom(ln_list *list, void *data, ln_cmp_func cmp) Find an element in a list, which can be compared with data by cmp returning 0. Return the element if it's found, else return NULL . ln_list *ln_list_find_all_custom(ln_list *list, void *data, ln_cmp_func cmp) Find all elements that can be compared with data by cmp returning 0. Return them in a new list. int ln_list_position(ln_list *list, ln_list *node) Return the position index of node in a list. int ln_list_index(ln_list *list, void *data) Return the position index of the first occurence of data in a list. data is compared directly by its pointer value. int ln_list_index_custom(ln_list *list, void *data, ln_cmp_func cmp) Return the position index of the first node whose element can be compared with data by cmp returning 0. int ln_list_length(ln_list *list) Return the length of a list. ln_list *ln_list_reverse(ln_list *list) Reverse the nodes in a list. ln_list *ln_list_from_array_size_t(size_t *array, size_t n) Create a list from an array with length n , whose elements are of type size_t . The elements if the list have the same pointer values as the array's elements' values. ln_list *ln_list_copy(ln_list *list) Create a new list, whose elements' values are the same as list 's elements' values.","title":"Data Structures"},{"location":"Documentation/Data-Structures.html#data-structures","text":"","title":"Data Structures"},{"location":"Documentation/Data-Structures.html#linked-list","text":"LightNet uses linked lists to manage a sequence of data structures, represented by ln_list , which is a simple singlely linked list. struct ln_list { void * data ; struct ln_list * next ; }; typedef struct ln_list ln_list ; ln_list doesn't have an empty header node, every node keeps real data. It has no \"create\" functions. Append an element to a NULL list to create a new list. Remember to always use the returned pointer as the newest list pointer, because the first node may have changed during updating (append, prepend, remove, insert, etc.). ln_list * list = NULL ; list = ln_list_append ( list , first_data ); ln_list supports the following operations: ln_list *ln_list_prepend(ln_list *list, void *data) Prepend an element at the beginning of the list. ln_list *ln_list_append(ln_list *list, void *data) Append an element at the end of the list. void ln_list_free(ln_list *list) Free all nodes in the list. void ln_list_free_deep(ln_list *list, void (*free_func)(void *)) Free all nodes int the list as well as their elements, which are freed by free_func . ln_list *ln_list_nth(ln_list *list, int n) Return the nth node in a list. void *ln_list_nth_data(ln_list *list, int n) Return the nth element in a list. ln_list *ln_list_remove(ln_list *list, void *data) Remove the first node that has data as its element in a list. data is compared directly by the pointer value. ln_list *ln_list_remove_nth(ln_list *list, int n) Remove the nth node. ln_list *ln_list_remove_nth_deep(ln_list *list, int n, void (*free_func)(void *)) Remove the nth node as well as its element, which is freed by free_func . ln_list *ln_list_remove_custom(ln_list *list, void *data, ln_cmp_func cmp) Remove the first node whose element can be compared with data by cmp returning 0. ln_list *ln_list_remove_custom_deep(ln_list *list, void *data, ln_cmp_func cmp, void (*free_func)(void *)) Remove the first node as well as its element which can be compared with data by cmp returning 0. The element is freed by free_func . ln_list *ln_list_remove_all_custom_deep(ln_list *list, void *data, ln_cmp_func cmp, void (*free_func)(void *)) Remove all the nodes as well as their elements which can be compared with data by cmp returning 0. The elements are freed by free_func . ln_list *ln_list_insert_before(ln_list *list, void *data, ln_list *node) Insert an element data before node in a list. ln_list *ln_list_insert_nth(ln_list *list, void *data, int n) Insert an element data at the nth position in a list. void *ln_list_find(ln_list *list, void *data) Find an element data in a list. data is compared directly by its pointer value. Return the element if it's found, else return NULL . void *ln_list_find_custom(ln_list *list, void *data, ln_cmp_func cmp) Find an element in a list, which can be compared with data by cmp returning 0. Return the element if it's found, else return NULL . ln_list *ln_list_find_all_custom(ln_list *list, void *data, ln_cmp_func cmp) Find all elements that can be compared with data by cmp returning 0. Return them in a new list. int ln_list_position(ln_list *list, ln_list *node) Return the position index of node in a list. int ln_list_index(ln_list *list, void *data) Return the position index of the first occurence of data in a list. data is compared directly by its pointer value. int ln_list_index_custom(ln_list *list, void *data, ln_cmp_func cmp) Return the position index of the first node whose element can be compared with data by cmp returning 0. int ln_list_length(ln_list *list) Return the length of a list. ln_list *ln_list_reverse(ln_list *list) Reverse the nodes in a list. ln_list *ln_list_from_array_size_t(size_t *array, size_t n) Create a list from an array with length n , whose elements are of type size_t . The elements if the list have the same pointer values as the array's elements' values. ln_list *ln_list_copy(ln_list *list) Create a new list, whose elements' values are the same as list 's elements' values.","title":"Linked List"},{"location":"Documentation/Intermediate-Representation.html","text":"Intermediate Representation \u00b6","title":"Intermediate Representation"},{"location":"Documentation/Intermediate-Representation.html#intermediate-representation","text":"","title":"Intermediate Representation"},{"location":"Documentation/Miscellaneous.html","text":"Miscellaneous \u00b6 An Easy-to-Write JSON Format \u00b6 For the convenience of writing JSON format files such as operator descriptions used in this project, we add several extensions to the official JSON format . The JSON extensions: The last elements in arrays and objects can have a trailing comma , . Comments are supported; every thing from double slashs // to the end of the line is a comment. String values can be heredoc-ed with two triple qoutes ''' , each in a seperated line, between which special characters can be auto-escaped, such as newlines, qoutes \" , etc. The triple qoutes must be at the end of a line except that the ending qoutes can have a comma following them. E.g: { \"some_array\" : [ { \"element1\" : 1 }, { \"element2\" : 2 }, ], // some very useful comments \"some_string\" : ''' No worries about \"strings\" and newlines here! ''' , } JSON files using above extensions can be easily transformed into official JSON files with lightnet/tools/easyjson.pl . For example, the above code would be transformed into the following JSON: { \"some_array\" : [ { \"element1\" : 1 }, { \"element2\" : 2 } ], \"some_string\" : \" No worries about \\\"strings\\\" and\\nnewlines here!\" } Tools \u00b6 Executables, scripts and modules helping the development and use of LightNet, located in lightnet/tools . You can always get help for a tool by typing tool_name -h . addop.pl Generate operator defination code from operator description JSON file. It supports the easy-to-write JSON format for its input. When used with -r ROOT option, it will save operator definations in ROOT/src/op/auto , and add operator declarations into corresponding ROOT/src/arch/ln_arch_*.c file. addtest.pl Generate test templates for a module. easyjson.pl A user interface for easyjson.pm . easyjson.pm A perl module with interfaces for transforming an easy-to-write JSON format file to an official JSON format file. genwts.pl A weight file generator. Convert the input text file containing weight numbers to one text file in hexadecimal string format. ir2json.pl Generate JSON-format IR code from input file which is in simplified IR format .","title":"Miscellaneous"},{"location":"Documentation/Miscellaneous.html#miscellaneous","text":"","title":"Miscellaneous"},{"location":"Documentation/Miscellaneous.html#an-easy-to-write-json-format","text":"For the convenience of writing JSON format files such as operator descriptions used in this project, we add several extensions to the official JSON format . The JSON extensions: The last elements in arrays and objects can have a trailing comma , . Comments are supported; every thing from double slashs // to the end of the line is a comment. String values can be heredoc-ed with two triple qoutes ''' , each in a seperated line, between which special characters can be auto-escaped, such as newlines, qoutes \" , etc. The triple qoutes must be at the end of a line except that the ending qoutes can have a comma following them. E.g: { \"some_array\" : [ { \"element1\" : 1 }, { \"element2\" : 2 }, ], // some very useful comments \"some_string\" : ''' No worries about \"strings\" and newlines here! ''' , } JSON files using above extensions can be easily transformed into official JSON files with lightnet/tools/easyjson.pl . For example, the above code would be transformed into the following JSON: { \"some_array\" : [ { \"element1\" : 1 }, { \"element2\" : 2 } ], \"some_string\" : \" No worries about \\\"strings\\\" and\\nnewlines here!\" }","title":"An Easy-to-Write JSON Format"},{"location":"Documentation/Miscellaneous.html#tools","text":"Executables, scripts and modules helping the development and use of LightNet, located in lightnet/tools . You can always get help for a tool by typing tool_name -h . addop.pl Generate operator defination code from operator description JSON file. It supports the easy-to-write JSON format for its input. When used with -r ROOT option, it will save operator definations in ROOT/src/op/auto , and add operator declarations into corresponding ROOT/src/arch/ln_arch_*.c file. addtest.pl Generate test templates for a module. easyjson.pl A user interface for easyjson.pm . easyjson.pm A perl module with interfaces for transforming an easy-to-write JSON format file to an official JSON format file. genwts.pl A weight file generator. Convert the input text file containing weight numbers to one text file in hexadecimal string format. ir2json.pl Generate JSON-format IR code from input file which is in simplified IR format .","title":"Tools"},{"location":"Documentation/Operator-Description.html","text":"Operator Description \u00b6","title":"Operator Description"},{"location":"Documentation/Operator-Description.html#operator-description","text":"","title":"Operator Description"},{"location":"Documentation/Optimizer-Description.html","text":"Optimizer Description \u00b6","title":"Optimizer Description"},{"location":"Documentation/Optimizer-Description.html#optimizer-description","text":"","title":"Optimizer Description"},{"location":"Documentation/Overview.html","text":"Overview \u00b6 LightNet is a light-weight neural network inference optimizer for different software/hardware backends. The deep learning algorithms for various application scenarios are becoming more and more mature. However, highly real-time algorithms in terminal devices such as Edge Computing Devices or Autonomous Agents (such as unmanned vehicles and drones) often require heterogeneous computing systems which are low-power and highly real-time. Such heterogeneous computing systems generally need to be developed with specific hardware-related languages \u200b\u200b(such as CUDA, OpenCL, Verilog, C/C++ etc.), which have the disadvantages of low development efficiency and long iteration period. In order to overcome those shortcomings, LightNet refers to traditional compiler optimization technologies, designs and develops a neural network optimizer for heterogeneous platforms, which can accept neural network models from various popular deep learning frameworks, optimize them according to the user's choice of backend platforms, and save the optimized models which can be directly run by LightNet. LightNet for now mainly focuses on the high-level optimization of a NN model, which can be completed with substitutions of the operators in a computing graph. Those optimizations are processed by the \"peephole optimizers\" of the corresponding backend, according to the optimization capabilities provided by the backend platforms. After the high-level optimizations, LightNet will plan the relative memory address used by each tensor, which can be converted to real memory address at runtime, at the same time keep the total memory usage as low as possible. LightNet essentially is an end-to-end NN optimizer framework that can be easily extended to support different model formats and backend computing platforms. However there are 3 diversification problems when designing such an optimizer which leads to enormous coding efforts: diversification of model formats, such as Tensorflow's checkpoint, Pytorch's pt. diversification of operators, operators are highly diverse and new operators are emerging. diversification of operator optimization capabilities of different backend platforms. LightNet provides 3 methods to solve those problems respectively: a unified model format and conversion tools for various frameworks an operator description language and code generation tools an optimizer description language and code generation tools Combined with the above methods, LightNet can be easily extended to support different model formats, all kinds of operators, and various backends with different optimization capabilities, to realize the rapid construction and automatic optimization of NN models. LightNet's core architecture is developed with C, which has the advantages of light weight, high efficieny and unified binary interface, which can be easiy combined with other languages and deployed in various hardware platforms.","title":"Overview"},{"location":"Documentation/Overview.html#overview","text":"LightNet is a light-weight neural network inference optimizer for different software/hardware backends. The deep learning algorithms for various application scenarios are becoming more and more mature. However, highly real-time algorithms in terminal devices such as Edge Computing Devices or Autonomous Agents (such as unmanned vehicles and drones) often require heterogeneous computing systems which are low-power and highly real-time. Such heterogeneous computing systems generally need to be developed with specific hardware-related languages \u200b\u200b(such as CUDA, OpenCL, Verilog, C/C++ etc.), which have the disadvantages of low development efficiency and long iteration period. In order to overcome those shortcomings, LightNet refers to traditional compiler optimization technologies, designs and develops a neural network optimizer for heterogeneous platforms, which can accept neural network models from various popular deep learning frameworks, optimize them according to the user's choice of backend platforms, and save the optimized models which can be directly run by LightNet. LightNet for now mainly focuses on the high-level optimization of a NN model, which can be completed with substitutions of the operators in a computing graph. Those optimizations are processed by the \"peephole optimizers\" of the corresponding backend, according to the optimization capabilities provided by the backend platforms. After the high-level optimizations, LightNet will plan the relative memory address used by each tensor, which can be converted to real memory address at runtime, at the same time keep the total memory usage as low as possible. LightNet essentially is an end-to-end NN optimizer framework that can be easily extended to support different model formats and backend computing platforms. However there are 3 diversification problems when designing such an optimizer which leads to enormous coding efforts: diversification of model formats, such as Tensorflow's checkpoint, Pytorch's pt. diversification of operators, operators are highly diverse and new operators are emerging. diversification of operator optimization capabilities of different backend platforms. LightNet provides 3 methods to solve those problems respectively: a unified model format and conversion tools for various frameworks an operator description language and code generation tools an optimizer description language and code generation tools Combined with the above methods, LightNet can be easily extended to support different model formats, all kinds of operators, and various backends with different optimization capabilities, to realize the rapid construction and automatic optimization of NN models. LightNet's core architecture is developed with C, which has the advantages of light weight, high efficieny and unified binary interface, which can be easiy combined with other languages and deployed in various hardware platforms.","title":"Overview"},{"location":"Documentation/Project-Structure.html","text":"Project Structure \u00b6 Directory Layout \u00b6 lightnet/docs Documentation files written in Markdown, which can be transformed into static web pages using mkdocs . lightnet/protos Prototype and description files: lightnet/protos/experiment Experimental prototypes used during development. lightnet/protos/net Network model files in IR format . lightnet/protos/op Operator description files for generating operator defination C source code. lightnet/src Core LightNet source code. lightnet/src/arch Source code for the definations of different software/hardware backends. lightnet/tools/addop.pl can automatically add operator declarations to the corresponding ln_arch_*.c files in this directory while generating operator defination code. lightnet/src/op Source code for the operator definations. lightnet/src/op/auto contains auto-generated operator defination code generated by lightnet/tools/addop.pl lightnet/test Feature and regression tests on LightNet infrastructure. Test templates can be generated with lightnet/tools/addtest.pl . lightnet/tools Executables and modules helping the development and use of LightNet. You can always get help for a tool by typing tool_name -h . See details in Tools . lightnet/common.mk Some common makefile code included by other makefiles. lightnet/config.mk Configuration variables generated by lightnet/configure . lightnet/configure Configuration script. Generate configuration variables in config.mk and check dependencies. Must be called before make . lightnet/Makefile Main makefile. lightnet/mkdocs.yml Configurations used by mkdocs for building documentation.","title":"Project Structure"},{"location":"Documentation/Project-Structure.html#project-structure","text":"","title":"Project Structure"},{"location":"Documentation/Project-Structure.html#directory-layout","text":"lightnet/docs Documentation files written in Markdown, which can be transformed into static web pages using mkdocs . lightnet/protos Prototype and description files: lightnet/protos/experiment Experimental prototypes used during development. lightnet/protos/net Network model files in IR format . lightnet/protos/op Operator description files for generating operator defination C source code. lightnet/src Core LightNet source code. lightnet/src/arch Source code for the definations of different software/hardware backends. lightnet/tools/addop.pl can automatically add operator declarations to the corresponding ln_arch_*.c files in this directory while generating operator defination code. lightnet/src/op Source code for the operator definations. lightnet/src/op/auto contains auto-generated operator defination code generated by lightnet/tools/addop.pl lightnet/test Feature and regression tests on LightNet infrastructure. Test templates can be generated with lightnet/tools/addtest.pl . lightnet/tools Executables and modules helping the development and use of LightNet. You can always get help for a tool by typing tool_name -h . See details in Tools . lightnet/common.mk Some common makefile code included by other makefiles. lightnet/config.mk Configuration variables generated by lightnet/configure . lightnet/configure Configuration script. Generate configuration variables in config.mk and check dependencies. Must be called before make . lightnet/Makefile Main makefile. lightnet/mkdocs.yml Configurations used by mkdocs for building documentation.","title":"Directory Layout"}]}