{
    "ops": [
        {
            "optype": "conv2d",
            "author": "Zhao Zhixu",
            "arch": "none",
            "tensors_in": [
                // [batch, channel, height, width]
                {"arg_name": "src", "mtype": "LN_MEM_NONE", "ndim": 4},
                // [output_channel, input_channel/group, height, width]
                {"arg_name": "weight", "mtype": "LN_MEM_NONE", "ndim": 4},
                {"arg_name": "bias", "mtype": "LN_MEM_NONE", "ndim": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(bias->dims[0] == weight->dims[0], "'bias' (%s) should have the size of dims[0] of 'weight' (%s)", ln_sprint_shape(shape1, bias->ndim, bias->dims), ln_sprint_shape(shape2, weight->ndim, weight->dims));
}
'''
                 }
            ],
            "tensors_out": [
                {"arg_name": "dst", "mtype": "LN_MEM_NONE",
                 "ndim": "src->ndim", "dtype": "src->dtype",
                 "custom": '''
{
dst_dims = ln_alloc(sizeof(int)*4);
dst_dims[0] = src->dims[0];
dst_dims[1] = weight->dims[0];
dst_dims[2] = ln_output_dim_conv(src->dims[2], size[0], stride[0], padding[0] + padding[2], dilation[0]);
dst_dims[3] = ln_output_dim_conv(src->dims[3], size[1], stride[1], padding[1] + padding[3], dilation[1]);
}
''',
                 "cleanup": "ln_free(dst_dims);"}
            ],
            "params": [
                {"arg_name": "group", "ptype": "LN_PARAM_NUMBER",
                 "realtype": "int", "ge": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(weight->dims[1]*group == src->dims[1], "'weight' (%s)'s dims[1] multiplies group (%d) should be equal to the dims[1] of 'src' (%s)", ln_sprint_shape(shape1, weight->ndim, weight->dims), group, ln_sprint_shape(shape2, src->ndim, src->dims));
}
'''
                 },
                // [height, width]
                {"arg_name": "size", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(size[0] == weight->dims[2] && size[1] == weight->dims[3], "'size' (%s) should be equal to the last two dimensions of 'weight' (%s)", ln_sprint_shape(shape1, size_entry->array_len, size), ln_sprint_shape(shape2, weight->ndim, weight->dims));
}
'''
                 },
                // [height, width]
                {"arg_name": "stride", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1},
                // [height, width]
                {"arg_name": "dilation", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1},
                // [top, left, bottom, right]
                {"arg_name": "padding", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 4, "ge": 0},
                {"arg_name": "autopad", "ptype": "LN_PARAM_STRING",
                 "custom": '''
{
if (ln_streq(autopad, "VALID") || ln_streq(autopad, "SAME_UPPER") ||
    ln_streq(autopad, "SAME_LOWER")) {
    ln_autopadding_conv(padding, &src->dims[2], size, stride, dilation, 2, autopad);
} else if (ln_streq(autopad, "NOTSET")){
} else {
    ln_msg_warn("unsupported 'autopad' %s", autopad);
}
}
'''
                },
            ]
        },
        {
            "optype": "conv2d_cpu",
            "author": "Zhao Zhixu",
            "arch": "cpu",
            "tensors_in": [
                // [batch, channel, height, width]
                {"arg_name": "src", "mtype": "LN_MEM_CPU", "ndim": 4},
                // [output_channel, input_channel/group, height, width]
                {"arg_name": "weight", "mtype": "LN_MEM_CPU", "ndim": 4},
                {"arg_name": "bias", "mtype": "LN_MEM_CPU", "ndim": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(bias->dims[0] == weight->dims[0], "'bias' (%s) should have the size of dims[0] of 'weight' (%s)", ln_sprint_shape(shape1, bias->ndim, bias->dims), ln_sprint_shape(shape2, weight->ndim, weight->dims));
}
'''
                 }
            ],
            "tensors_out": [
                {"arg_name": "dst", "mtype": "LN_MEM_CPU",
                 "ndim": "src->ndim", "dtype": "src->dtype",
                 "custom": '''
{
dst_dims = ln_alloc(sizeof(int)*4);
dst_dims[0] = src->dims[0];
dst_dims[1] = weight->dims[0];
dst_dims[2] = ln_output_dim_conv(src->dims[2], size[0], stride[0], padding[0] + padding[2], dilation[0]);
dst_dims[3] = ln_output_dim_conv(src->dims[3], size[1], stride[1], padding[1] + padding[3], dilation[1]);
}
''',
                 "cleanup": "ln_free(dst_dims);"}
            ],
            "params": [
                {"arg_name": "group", "ptype": "LN_PARAM_NUMBER",
                 "realtype": "int", "ge": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(weight->dims[1]*group == src->dims[1], "'weight' (%s)'s dims[1] multiplies group (%d) should be equal to the dims[1] of 'src' (%s)", ln_sprint_shape(shape1, weight->ndim, weight->dims), group, ln_sprint_shape(shape2, src->ndim, src->dims));
}
'''
                 },
                // [height, width]
                {"arg_name": "size", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(size[0] == weight->dims[2] && size[1] == weight->dims[3], "'size' (%s) should be equal to the last two dimensions of 'weight' (%s)", ln_sprint_shape(shape1, size_entry->array_len, size), ln_sprint_shape(shape2, weight->ndim, weight->dims));
}
'''
                 },
                // [height, width]
                {"arg_name": "stride", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1},
                // [height, width]
                {"arg_name": "dilation", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1},
                // [top, left, bottom, right]
                {"arg_name": "padding", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 4, "ge": 0},
                {"arg_name": "autopad", "ptype": "LN_PARAM_STRING",
                 "custom": '''
{
if (ln_streq(autopad, "VALID") || ln_streq(autopad, "SAME_UPPER") ||
    ln_streq(autopad, "SAME_LOWER")) {
    ln_autopadding_conv(padding, &src->dims[2], size, stride, dilation, 2, autopad);
} else if (ln_streq(autopad, "NOTSET")){
} else {
    ln_msg_warn("unsupported 'autopad' %s", autopad);
}
}
'''
                },
            ],
            "run": ""
        },
        {
            "optype": "conv2d_cuda",
            "author": "Zhao Zhixu",
            "arch": "cuda",
            "tensors_in": [
                // [batch, channel, height, width]
                {"arg_name": "src", "mtype": "LN_MEM_CUDA", "ndim": 4},
                // [output_channel, input_channel/group, height, width]
                {"arg_name": "weight", "mtype": "LN_MEM_CUDA", "ndim": 4},
                {"arg_name": "bias", "mtype": "LN_MEM_CUDA", "ndim": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(bias->dims[0] == weight->dims[0], "'bias' (%s) should have the size of dims[0] of 'weight' (%s)", ln_sprint_shape(shape1, bias->ndim, bias->dims), ln_sprint_shape(shape2, weight->ndim, weight->dims));
}
'''
                 }
            ],
            "tensors_out": [
                {"arg_name": "dst", "mtype": "LN_MEM_CUDA",
                 "ndim": "src->ndim", "dtype": "src->dtype",
                 "custom": '''
{
dst_dims = ln_alloc(sizeof(int)*4);
dst_dims[0] = src->dims[0];
dst_dims[1] = weight->dims[0];
dst_dims[2] = ln_output_dim_conv(src->dims[2], size[0], stride[0], padding[0] + padding[2], dilation[0]);
dst_dims[3] = ln_output_dim_conv(src->dims[3], size[1], stride[1], padding[1] + padding[3], dilation[1]);
}
''',
                 "cleanup": "ln_free(dst_dims);"}
            ],
            "params": [
                {"arg_name": "group", "ptype": "LN_PARAM_NUMBER",
                 "realtype": "int", "ge": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(weight->dims[1]*group == src->dims[1], "'weight' (%s)'s dims[1] multiplies group (%d) should be equal to the dims[1] of 'src' (%s)", ln_sprint_shape(shape1, weight->ndim, weight->dims), group, ln_sprint_shape(shape2, src->ndim, src->dims));
}
'''
                 },
                // [height, width]
                {"arg_name": "size", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1,
                 "custom": '''
{
    char shape1[LN_MAXLINE];
    char shape2[LN_MAXLINE];
    ln_opck_satisfy_msg(size[0] == weight->dims[2] && size[1] == weight->dims[3], "'size' (%s) should be equal to the last two dimensions of 'weight' (%s)", ln_sprint_shape(shape1, size_entry->array_len, size), ln_sprint_shape(shape2, weight->ndim, weight->dims));
}
'''
                 },
                // [height, width]
                {"arg_name": "stride", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1},
                // [height, width]
                {"arg_name": "dilation", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 2, "ge": 1},
                // [top, left, bottom, right]
                {"arg_name": "padding", "ptype": "LN_PARAM_ARRAY_NUMBER",
                 "realtype": "int", "len": 4, "ge": 0},
                {"arg_name": "autopad", "ptype": "LN_PARAM_STRING",
                 "custom": '''
{
if (ln_streq(autopad, "VALID") || ln_streq(autopad, "SAME_UPPER") ||
    ln_streq(autopad, "SAME_LOWER")) {
    ln_autopadding_conv(padding, &src->dims[2], size, stride, dilation, 2, autopad);
} else if (ln_streq(autopad, "NOTSET")){
} else {
    ln_msg_warn("unsupported 'autopad' %s", autopad);
}
}
'''
                },
            ],
        "run": ""
        },
    ]
}
